{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurpeter/Machine_Learning/blob/main/Notiuni_fundamentale_de_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410a231e",
      "metadata": {
        "id": "410a231e"
      },
      "source": [
        "#### Setul de date\n",
        "\n",
        "Bolile cardiovasculare(BCV) sunt cea mai importantă cauză de deces la nivel global, provocând aproximativ 17,9 milioane de vieți pierdute în fiecare an, ceea ce reprezintă 31% din toate decesele la nivel mondial. Insuficiența cardiacă este un eveniment comun cauzat de BCV, iar acest set de date conține 12 caracteristici care pot fi utilizate pentru a prezice mortalitatea cauzată de insuficiența cardiacă.\n",
        "\n",
        "Cele mai multe boli cardiovasculare pot fi prevenite prin abordarea factorilor de risc comportamentali, cum ar fi consumul de tutun, dieta nesănătoasă și obezitatea, inactivitatea fizică și consumul nociv de alcool, prin strategii la nivel de populație.\n",
        "\n",
        "Persoanele cu boli cardiovasculare sau cele care au un risc cardiovascular ridicat (datorită prezenței unuia sau mai multor factori de risc, cum ar fi hipertensiunea, diabetul, hiperlipidemia sau o boală deja stabilită) au nevoie de detectare timpurie și gestionare, în care un model de învățare automată poate fi de mare ajutor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faae5295",
      "metadata": {
        "id": "faae5295"
      },
      "outputs": [],
      "source": [
        "!unzip Data.zip -d Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bf2b82",
      "metadata": {
        "id": "41bf2b82"
      },
      "source": [
        "### Supervised Learning\n",
        "Clasificare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeafd7c",
      "metadata": {
        "id": "4eeafd7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38db536c",
      "metadata": {
        "id": "38db536c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Data/heart_failure_clinical_records_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b57e4d2",
      "metadata": {
        "id": "1b57e4d2"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1aa3f29",
      "metadata": {
        "id": "e1aa3f29"
      },
      "outputs": [],
      "source": [
        "print(f'Setul de date are {df.shape[0]} randuri si {df.shape[1]} coloane')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9f8c79",
      "metadata": {
        "id": "0e9f8c79"
      },
      "source": [
        "#### Impartirea datelor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611a7658",
      "metadata": {
        "id": "611a7658"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
        "train_df = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48756be4",
      "metadata": {
        "id": "48756be4"
      },
      "source": [
        "#### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aff7d9b",
      "metadata": {
        "id": "5aff7d9b"
      },
      "outputs": [],
      "source": [
        "descriptive = pd.DataFrame(round(train_df.describe(include='all'),3).T)\n",
        "\n",
        "descriptive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e63f0b",
      "metadata": {
        "id": "b0e63f0b"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(4,4, figsize=(20,20))\n",
        "\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i,var in enumerate(train_df.columns):\n",
        "\n",
        "    sns.histplot(data=train_df, x=var, ax=ax[i])\n",
        "\n",
        "    ax[i].set_title(var)\n",
        "\n",
        "for j in range(len(train_df.columns), len(ax)):\n",
        "    fig.delaxes(ax[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35693983",
      "metadata": {
        "id": "35693983"
      },
      "outputs": [],
      "source": [
        "num_vars = train_df.columns.difference(['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT'])\n",
        "\n",
        "cat_vars = train_df.columns.difference(num_vars)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(3,3, figsize=(20,20))\n",
        "\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i, var in enumerate(num_vars):\n",
        "\n",
        "    sns.boxplot(data=train_df, x=var, ax=ax[i])\n",
        "\n",
        "    ax[i].set_title(f'The boxplot of {var}')\n",
        "\n",
        "for j in range(len(num_vars), len(ax)):\n",
        "    fig.delaxes(ax[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a42bb2fd",
      "metadata": {
        "scrolled": true,
        "id": "a42bb2fd"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3,3,figsize=(20,20))\n",
        "\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i, var in enumerate(num_vars):\n",
        "    sns.kdeplot(data=train_df, x=var, hue='DEATH_EVENT', ax=ax[i])\n",
        "    ax[i].set_title(f'The distribution of {var}')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe9a4e6",
      "metadata": {
        "id": "7fe9a4e6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2,3, figsize=(16,12))\n",
        "\n",
        "ax = ax.flatten()\n",
        "\n",
        "cat_vars = train_df.columns.difference(num_vars).difference(['DEATH_EVENT'])\n",
        "\n",
        "for i, var in enumerate(cat_vars):\n",
        "    pd.crosstab(train_df[var], train_df['DEATH_EVENT'], normalize='index').plot(kind='bar', stacked=True, ax=ax[i])\n",
        "    ax[i].set_title(f'P(Heart stroke given {var})')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c73a58b",
      "metadata": {
        "id": "9c73a58b"
      },
      "source": [
        "Checking for correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a458b4",
      "metadata": {
        "id": "69a458b4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(round(train_df.corr(),2), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0212ed",
      "metadata": {
        "id": "3d0212ed"
      },
      "source": [
        "#### PCA\n",
        "Principal component analysis (PCA) reduce numărul de dimensiuni în seturi de date la componente principale care păstrează cea mai mare parte a informației originale. Acest lucru se realizează prin transformarea variabilelor potențial corelate într-un set mai mic de variabile, numit componente principale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf16e794",
      "metadata": {
        "id": "bf16e794"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f104630",
      "metadata": {
        "id": "0f104630"
      },
      "source": [
        "StandardScaler: Acesta este utilizat pentru a standardiza datele prin scalarea caracteristicilor astfel încât să aibă o medie de 0 și o deviație standard de 1. Standardizarea este esențială pentru PCA, care este sensibil la scara caracteristicilor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e142d0",
      "metadata": {
        "id": "c0e142d0"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensions\n",
        "reduced_variables = ['time','ejection_fraction','age','serum_creatinine','creatinine_phosphokinase','sex','smoking','serum_sodium','high_blood_pressure']\n",
        "pca_ready = train_df[reduced_variables]\n",
        "\n",
        "# Scaled data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "pca_ready = scaler.fit_transform(pca_ready)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d37492c",
      "metadata": {
        "id": "4d37492c"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "\n",
        "train_pca_fitted = pca.fit_transform(pca_ready)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e96e4b2",
      "metadata": {
        "id": "4e96e4b2"
      },
      "outputs": [],
      "source": [
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "\n",
        "sns.lineplot(x= range(1,len(cumulative_variance)+1), y=cumulative_variance, ax=ax, marker='o')\n",
        "\n",
        "plt.title(f'Cumulative variance explained by components')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10739d5c",
      "metadata": {
        "id": "10739d5c"
      },
      "outputs": [],
      "source": [
        "#Refit with n_components = 6 to capture 80% variance of original training data\n",
        "\n",
        "pca = PCA(n_components=6)\n",
        "\n",
        "train_pca_fitted = pca.fit_transform(pca_ready)\n",
        "\n",
        "test_pca_fitted = pca.transform(scaler.transform(X_test[reduced_variables]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eaf58e6",
      "metadata": {
        "id": "4eaf58e6"
      },
      "outputs": [],
      "source": [
        "train_pca_fitted = pd.DataFrame(train_pca_fitted, columns=['PC1','PC2','PC3','PC4','PC5','PC6'])\n",
        "\n",
        "train_pca_fitted['DEATH_EVENT'] = train_df['DEATH_EVENT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247d4766",
      "metadata": {
        "id": "247d4766"
      },
      "outputs": [],
      "source": [
        "train_pca_fitted.corr()\n",
        "\n",
        "sns.heatmap(round(train_pca_fitted.corr(),2), annot=True, cmap='coolwarm')\n",
        "\n",
        "plt.title('Correlation Matrix')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3939ccd",
      "metadata": {
        "id": "c3939ccd"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=train_pca_fitted, hue='DEATH_EVENT')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace5236c",
      "metadata": {
        "id": "ace5236c"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0ac322",
      "metadata": {
        "id": "5d0ac322"
      },
      "outputs": [],
      "source": [
        "premodel_scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = premodel_scaler.fit_transform(X_train[reduced_variables])\n",
        "X_test_scaled = premodel_scaler.transform(X_test[reduced_variables])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074bdebc",
      "metadata": {
        "id": "074bdebc"
      },
      "outputs": [],
      "source": [
        "X_train_scaled.shape, X_test_scaled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63204aa9",
      "metadata": {
        "id": "63204aa9"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae31b48",
      "metadata": {
        "id": "7ae31b48"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b877b002",
      "metadata": {
        "id": "b877b002"
      },
      "source": [
        "### Exercițiul 1\n",
        "\n",
        "Definiti urmatoarele modele de clasificare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2b6afe",
      "metadata": {
        "id": "de2b6afe"
      },
      "outputs": [],
      "source": [
        "Models = {\n",
        "\n",
        "    'Logistic Regression': LogisticRegression(penalty='l2', C=0.02),\n",
        "\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=7),\n",
        "\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=500, max_depth=10),\n",
        "\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa59443",
      "metadata": {
        "id": "cfa59443"
      },
      "source": [
        "### Exercițiul 2\n",
        "\n",
        "a) Antrenează modelul: Folosește metoda fit() pentru a antrena modelul specificat pe datele de antrenament scalate, X_train_scaled și etichetele de antrenament, y_train.\n",
        "\n",
        "b) Utilizează metoda predict() pentru a genera predicții pe setul de testare scalat, X_test_scaled.\n",
        "\n",
        "c) Stochează în dicționarul results numele modelului ca cheie și un tuplu care conține recall-ul și precision-ul obținut pe setul de testare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424915a5",
      "metadata": {
        "id": "424915a5"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for model_name, model in Models.items():\n",
        "\n",
        "    # TO DO a)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    results[model_name] = (recall_score(y_test, y_pred), precision_score(y_test, y_pred))\n",
        "\n",
        "print(\"Recall, Precision\")\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8a2deb",
      "metadata": {
        "id": "fd8a2deb"
      },
      "outputs": [],
      "source": [
        "# Results on original data without PCA\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in Models.items():\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    results[model_name] = (recall_score(y_test, y_pred, pos_label=1), precision_score(y_test, y_pred, pos_label=1))\n",
        "\n",
        "print(\"Recall, Precision\")\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7284c46",
      "metadata": {
        "id": "b7284c46"
      },
      "source": [
        "L1 regularization (Lasso regularization) adds a penalty equal to the absolute values of the coefficients, which can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "L2 regularization (Ridge regularization) adds a penalty equal to the sum of the squares of the coefficients. This discourages large coefficient values, effectively shrinking them towards zero without making them exactly zero. L2 regularization generally helps in cases where the features have a moderate level of collinearity.\n",
        "\n",
        "C=0.05 controls the strength of regularization. Smaller values of C mean stronger regularization.\n",
        "In general, this configuration helps produce a simpler model that generalizes better to new data, especially when working with datasets where there is a risk of overfitting or noisy features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a307f6",
      "metadata": {
        "id": "04a307f6"
      },
      "outputs": [],
      "source": [
        "best_model = LogisticRegression(penalty='l2',C=.02)\n",
        "best_model.fit(X_train_scaled, y_train)\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(f'Precision: {precision_score(y_test, y_pred, pos_label=1)}')\n",
        "print(f'Recall: {recall_score(y_test, y_pred, pos_label=1)}')\n",
        "print(f'F1 score: {f1_score(y_test, y_pred, pos_label=1)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef02fae",
      "metadata": {
        "id": "4ef02fae"
      },
      "source": [
        "### Exercițiul 3\n",
        "Afisati matricea de confuzie pentru predictia realizata cu modelul best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029b5e02",
      "metadata": {
        "id": "029b5e02"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# TO DO\n",
        "cm = confusion_matrix(y_test, y_pred, labels=best_model.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32a32c1",
      "metadata": {
        "id": "c32a32c1"
      },
      "source": [
        "### Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da08fae1",
      "metadata": {
        "id": "da08fae1"
      },
      "source": [
        "Diferite tipuri de optimizatori, functii de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f1bc6d",
      "metadata": {
        "id": "59f1bc6d"
      },
      "outputs": [],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53de181",
      "metadata": {
        "id": "f53de181"
      },
      "outputs": [],
      "source": [
        "import os # A module that provides a way to interact with the operating system, allowing for tasks such as file and directory manipulation.\n",
        "import torch\n",
        "from PIL import Image # A module from the Python Imaging Library (PIL) that provides functionality for opening, manipulating, and saving various image file formats.\n",
        "from torch.utils.data import Dataset # A PyTorch class that represents a dataset and provides an interface for accessing and processing the data during training.\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms # A module from the torchvision library that provides common image transformations, such as resizing, cropping, and normalization.\n",
        "from torch.utils.data import random_split # A function from PyTorch that allows for randomly splitting a dataset into training and validation subsets.\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # A PyTorch scheduler that adjusts the learning rate during training based on a specified metric, reducing it when the metric plateaus.\n",
        "import torch.nn as nn # A module in PyTorch that provides classes for defining and building neural networks.\n",
        "from torchvision import utils # A module from torchvision that contains utility functions for working with images, such as saving and visualizing them.\n",
        "from torchvision.datasets import ImageFolder\n",
        "import splitfolders\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import pathlib\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools # This import statement imports the itertools module, which provides functions for efficient looping and combining of iterables. It can be used for tasks such as generating combinations or permutations of elements.\n",
        "from tqdm.notebook import trange, tqdm # These functions allow for the creation of progress bars to track the progress of loops or tasks.\n",
        "from torch import optim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1fda43f",
      "metadata": {
        "id": "a1fda43f"
      },
      "source": [
        "#### Dataset\n",
        "Citirea datelor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1c99aa",
      "metadata": {
        "id": "5d1c99aa"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv('Data/metadata.csv')\n",
        "print(labels_df.head().to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f03be4ca",
      "metadata": {
        "id": "f03be4ca"
      },
      "outputs": [],
      "source": [
        "os.listdir('Data/Brain Tumor Data Set')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a026086e",
      "metadata": {
        "id": "a026086e"
      },
      "outputs": [],
      "source": [
        "labels_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dbea732",
      "metadata": {
        "id": "6dbea732"
      },
      "source": [
        "Procesarea setului de date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8830e0",
      "metadata": {
        "id": "bb8830e0"
      },
      "outputs": [],
      "source": [
        "data_dir = 'Data/Brain Tumor Data Set'\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "splitfolders.ratio(data_dir, output='brain', seed=20, ratio=(0.8, 0.2))\n",
        "\n",
        "data_dir = 'brain'\n",
        "data_dir = pathlib.Path(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568e1fb6",
      "metadata": {
        "id": "568e1fb6"
      },
      "source": [
        "### Exercițiul 4\n",
        "Creează o variabilă numită transform, care să conțină o secvență de transformări aplicate fiecărei imagini, după cum urmează:\n",
        "\n",
        "Redimensionare: Redimensionează fiecare imagine la 256x256 pixeli.\n",
        "\n",
        "Flip orizontal: Aplică o întoarcere orizontală aleatorie, cu probabilitatea de 50%.\n",
        "\n",
        "Flip vertical: Aplică o întoarcere verticală aleatorie, cu probabilitatea de 50%.\n",
        "\n",
        "Rotire aleatorie: Aplică o rotație aleatorie în intervalul de ±30 de grade.\n",
        "\n",
        "Conversie în tensor: Convertește imaginea într-un tensor utilizabil de PyTorch.\n",
        "\n",
        "Normalizare: Normalizează valorile pixelilor folosind media [0.485, 0.456, 0.406] și deviația standard [0.229, 0.224, 0.225], pentru a asigura compatibilitatea cu un model pre-antrenat.\n",
        "\n",
        "Folosește transforms.Compose() pentru a crea acest pipeline de transformări și asignează-l variabilei transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361b5bb0",
      "metadata": {
        "id": "361b5bb0"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "      transforms.Resize((256, 256)),\n",
        "      transforms.RandomHorizontalFlip(p=0.5),\n",
        "      transforms.RandomVerticalFlip(p=0.5),\n",
        "      transforms.RandomRotation(degrees=30),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a2d516",
      "metadata": {
        "id": "06a2d516"
      },
      "outputs": [],
      "source": [
        "train_set = torchvision.datasets.ImageFolder(data_dir.joinpath(\"train\"), transform=transform)\n",
        "train_set.transform\n",
        "val_set = torchvision.datasets.ImageFolder(data_dir.joinpath(\"val\"), transform=transform)\n",
        "val_set.transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3713df",
      "metadata": {
        "id": "6d3713df"
      },
      "outputs": [],
      "source": [
        "CLA_label = {\n",
        "    0 : 'Brain Tumor',\n",
        "    1 : 'Healthy'\n",
        "}\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "cols, rows = 4, 4\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
        "    img, label = train_set[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(CLA_label[label])\n",
        "    plt.axis(\"off\")\n",
        "    img_np = img.numpy().transpose((1, 2, 0))\n",
        "    # Clip pixel values to [0, 1]\n",
        "    img_valid_range = np.clip(img_np, 0, 1)\n",
        "    plt.imshow(img_valid_range)\n",
        "    plt.suptitle('Brain Images', y=0.95)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98474840",
      "metadata": {
        "id": "98474840"
      },
      "source": [
        "Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a4b4db",
      "metadata": {
        "id": "e2a4b4db"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle = True, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204170e5",
      "metadata": {
        "id": "204170e5"
      },
      "outputs": [],
      "source": [
        "for key, value in {'Training data': train_loader, \"Validation data\": val_loader}.items():\n",
        "    for X, y in value:\n",
        "        print(f\"{key}:\")\n",
        "        print(f\"Shape of X : {X.shape}\")\n",
        "        print(f\"Shape of y: {y.shape} {y.dtype}\\n\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d42dd61",
      "metadata": {
        "id": "9d42dd61"
      },
      "source": [
        "#### Arhitectura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4ee739",
      "metadata": {
        "id": "0c4ee739"
      },
      "outputs": [],
      "source": [
        "'''This function can be useful in determining the output size of a convolutional layer in a neural network,\n",
        "given the input dimensions and the convolutional layer's parameters.'''\n",
        "\n",
        "def findConv2dOutShape(hin,win,conv,pool=2):\n",
        "    # get conv arguments\n",
        "    kernel_size = conv.kernel_size\n",
        "    stride=conv.stride\n",
        "    padding=conv.padding\n",
        "    dilation=conv.dilation\n",
        "\n",
        "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
        "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
        "\n",
        "    if pool:\n",
        "        hout/=pool\n",
        "        wout/=pool\n",
        "    return int(hout),int(wout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3cdbb5",
      "metadata": {
        "id": "3d3cdbb5"
      },
      "outputs": [],
      "source": [
        "class CNN_TUMOR(nn.Module):\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        super(CNN_TUMOR, self).__init__()\n",
        "\n",
        "        Cin,Hin,Win = params[\"shape_in\"]\n",
        "        init_f = params[\"initial_filters\"]\n",
        "        num_fc1 = params[\"num_fc1\"]\n",
        "        num_classes = params[\"num_classes\"]\n",
        "        self.dropout_rate = params[\"dropout_rate\"]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(Cin, init_f, kernel_size=3)\n",
        "        h,w=findConv2dOutShape(Hin,Win,self.conv1)\n",
        "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
        "        h,w=findConv2dOutShape(h,w,self.conv2)\n",
        "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
        "        h,w=findConv2dOutShape(h,w,self.conv3)\n",
        "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
        "        h,w=findConv2dOutShape(h,w,self.conv4)\n",
        "\n",
        "        self.num_flatten=h*w*8*init_f\n",
        "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
        "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
        "\n",
        "    def forward(self,X):\n",
        "\n",
        "        X = F.relu(self.conv1(X));\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = F.relu(self.conv4(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = X.view(-1, self.num_flatten)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.dropout(X, self.dropout_rate)\n",
        "        X = self.fc2(X)\n",
        "        return F.log_softmax(X, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ef96ea",
      "metadata": {
        "id": "e7ef96ea"
      },
      "source": [
        "### Exercițiul 5\n",
        "Instanțiați rețeaua și asigurați-vă că aceasta rulează pe un GPU, dacă acesta este disponibil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c9d5b1",
      "metadata": {
        "id": "21c9d5b1"
      },
      "outputs": [],
      "source": [
        "params_model={\n",
        "        \"shape_in\": (3,256,256),\n",
        "        \"initial_filters\": 8,\n",
        "        \"num_fc1\": 100,\n",
        "        \"dropout_rate\": 0.25,\n",
        "        \"num_classes\": 2}\n",
        "\n",
        "cnn_model = CNN_TUMOR(params_model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = cnn_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b8d428",
      "metadata": {
        "id": "81b8d428"
      },
      "outputs": [],
      "source": [
        "summary(cnn_model, input_size=(3, 256, 256),device=device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e45178e",
      "metadata": {
        "id": "2e45178e"
      },
      "source": [
        "### Exercițiul 6\n",
        "Definiți funcția de pierdere (*loss function*) și optimizatorul Adam cu valoarea parametrului learning rate = 3e-4 pentru antrenarea modelului."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0ea77d",
      "metadata": {
        "id": "9d0ea77d"
      },
      "outputs": [],
      "source": [
        "loss_func =  nn.NLLLoss(reduction=\"sum\")# Definti negative log likelihood loss cu parametrul reduction=\"sum\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a88468",
      "metadata": {
        "id": "61a88468"
      },
      "outputs": [],
      "source": [
        "opt = optim.AdamW(model.parameters(), lr=0.001) # Adam optimizer\n",
        "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b613f70f",
      "metadata": {
        "id": "b613f70f"
      },
      "outputs": [],
      "source": [
        "# Function to get the learning rate\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# Function to compute the loss value per batch of data\n",
        "def loss_batch(loss_func, output, target, opt=None):\n",
        "\n",
        "    loss = loss_func(output, target) # get loss\n",
        "    pred = output.argmax(dim=1, keepdim=True) # Get Output Class\n",
        "    metric_b=pred.eq(target.view_as(pred)).sum().item() # get performance metric\n",
        "\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "# Compute the loss value & performance metric for the entire dataset (epoch)\n",
        "def loss_epoch(model,loss_func,dataset_dl,opt=None):\n",
        "\n",
        "    run_loss=0.0\n",
        "    t_metric=0.0\n",
        "    len_data=len(dataset_dl.dataset)\n",
        "\n",
        "    # internal loop over dataset\n",
        "    for xb, yb in dataset_dl:\n",
        "        # move batch to device\n",
        "        xb=xb.to(device)\n",
        "        yb=yb.to(device)\n",
        "        output=model(xb) # get model output\n",
        "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # get loss per batch\n",
        "        run_loss+=loss_b        # update running loss\n",
        "\n",
        "        if metric_b is not None: # update running metric\n",
        "            t_metric+=metric_b\n",
        "\n",
        "    loss=run_loss/float(len_data)  # average loss value\n",
        "    metric=t_metric/float(len_data) # average metric value\n",
        "\n",
        "    return loss, metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9593050d",
      "metadata": {
        "id": "9593050d"
      },
      "outputs": [],
      "source": [
        "def Train_Val(model, params,verbose=False):\n",
        "\n",
        "    # Get the parameters\n",
        "    epochs=params[\"epochs\"]\n",
        "    loss_func=params[\"f_loss\"]\n",
        "    opt=params[\"optimiser\"]\n",
        "    train_dl=params[\"train\"]\n",
        "    val_dl=params[\"val\"]\n",
        "    lr_scheduler=params[\"lr_change\"]\n",
        "    weight_path=params[\"weight_path\"]\n",
        "\n",
        "    # history of loss values in each epoch\n",
        "    loss_history={\"train\": [],\"val\": []}\n",
        "    # histroy of metric values in each epoch\n",
        "    metric_history={\"train\": [],\"val\": []}\n",
        "    # a deep copy of weights for the best performing model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    # initialize best loss to a large value\n",
        "    best_loss=float('inf')\n",
        "\n",
        "# Train Model n_epochs (the progress of training by printing the epoch number and the associated learning rate. It can be helpful for debugging, monitoring the learning rate schedule, or gaining insights into the training process.)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        # Get the Learning Rate\n",
        "        current_lr=get_lr(opt)\n",
        "        if(verbose):\n",
        "            print('Epoch {}/{}, current lr={}'.format(epoch, epochs - 1, current_lr))\n",
        "\n",
        "\n",
        "# Train Model Process\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,opt)\n",
        "\n",
        "        # collect losses\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "        metric_history[\"train\"].append(train_metric)\n",
        "\n",
        "\n",
        "# Evaluate Model Process\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model,loss_func,val_dl)\n",
        "\n",
        "        # store best model\n",
        "        if(val_loss < best_loss):\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            # store weights into a local file\n",
        "            torch.save(model.state_dict(), weight_path)\n",
        "            if(verbose):\n",
        "                print(\"Copied best model weights!\")\n",
        "\n",
        "        # collect loss and metric for validation dataset\n",
        "        loss_history[\"val\"].append(val_loss)\n",
        "        metric_history[\"val\"].append(val_metric)\n",
        "\n",
        "        # learning rate schedule\n",
        "        lr_scheduler.step(val_loss)\n",
        "        if current_lr != get_lr(opt):\n",
        "            if(verbose):\n",
        "                print(\"Loading best model weights!\")\n",
        "            model.load_state_dict(best_model_wts)\n",
        "\n",
        "        if(verbose):\n",
        "            print(f\"train loss: {train_loss:.6f}, dev loss: {val_loss:.6f}, accuracy: {100*val_metric:.2f}\")\n",
        "            print(\"-\"*10)\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, loss_history, metric_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a049a879",
      "metadata": {
        "id": "a049a879"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# Define various parameters used for training and evaluation of a cnn_model\n",
        "\n",
        "params_train={\n",
        " \"train\": train_loader,\"val\": val_loader,\n",
        " \"epochs\": 20,\n",
        " \"optimiser\": optim.AdamW(cnn_model.parameters(),lr=3e-4),\n",
        " \"lr_change\": ReduceLROnPlateau(opt,\n",
        "                                mode='min',\n",
        "                                factor=0.5,\n",
        "                                patience=20),\n",
        " \"f_loss\": nn.NLLLoss(reduction=\"sum\"),\n",
        " \"weight_path\": \"weights.pt\",\n",
        "}\n",
        "\n",
        "# train and validate the model\n",
        "cnn_model,loss_hist,metric_hist = Train_Val(cnn_model,params_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6072484",
      "metadata": {
        "id": "b6072484"
      },
      "outputs": [],
      "source": [
        "# Convergence History Plot\n",
        "epochs=params_train[\"epochs\"]\n",
        "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
        "\n",
        "sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n",
        "sns.lineplot(x=[*range(1,epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n",
        "sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"train\"],ax=ax[1],label='Acc_hist[\"train\"]')\n",
        "sns.lineplot(x=[*range(1,epochs+1)],y=metric_hist[\"val\"],ax=ax[1],label='Acc_hist[\"val\"]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e1490a",
      "metadata": {
        "id": "88e1490a"
      },
      "outputs": [],
      "source": [
        "# define function For Classification Report\n",
        "def Ture_and_Pred(val_loader, model):\n",
        "    i = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.numpy()\n",
        "        outputs = model(images)\n",
        "        _, pred = torch.max(outputs.data, 1)\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "\n",
        "        y_true = np.append(y_true, labels)\n",
        "        y_pred = np.append(y_pred, pred)\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "# check confusion matrix for error analysis\n",
        "y_true, y_pred = Ture_and_Pred(val_loader, cnn_model)\n",
        "\n",
        "print(classification_report(y_true, y_pred), '\\n\\n')\n",
        "cm = confusion_matrix(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f945ce1b",
      "metadata": {
        "id": "f945ce1b"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix Plotting Function\n",
        "def show_confusion_matrix(cm, CLA_label, title='Confusion matrix', cmap=plt.cm.YlGnBu):\n",
        "\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.grid(False)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(CLA_label))\n",
        "\n",
        "    plt.xticks(tick_marks, [f\"{value}={key}\" for key , value in CLA_label.items()], rotation=45)\n",
        "    plt.yticks(tick_marks, [f\"{value}={key}\" for key , value in CLA_label.items()])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, f\"{cm[i,j]}\\n{cm[i,j]/np.sum(cm)*100:.2f}%\", horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_confusion_matrix(cm, CLA_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1918712c",
      "metadata": {
        "id": "1918712c"
      },
      "outputs": [],
      "source": [
        "torch.save(cnn_model, \"Brain_Tumor_model.pt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}