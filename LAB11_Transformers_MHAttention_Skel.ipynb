{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurpeter/Machine_Learning/blob/main/LAB11_Transformers_MHAttention_Skel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKE2vjx7PUG"
      },
      "source": [
        "## Lab 11 Transformers - Multi Head Attention\n",
        "\n",
        "În acest tutorial, vom discuta despre una dintre cele mai influente arhitecturi din ultimii ani: modelul Transformer. De la publicarea lucrării [Attention Is All You Need](https://arxiv.org/abs/1706.03762) de către Vaswani și colaboratorii săi în 2017, arhitectura Transformer a continuat să fundamenteze modele neurale în multe domenii, cel mai important în procesarea limbajului natural (NLP). Transformerele cu un număr mare de parametri pot genera [eseuri](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3) lungi și convingătoare și au deschis noi domenii de aplicare ale inteligenței artificiale. Deoarece entuziasmul pentru arhitectura Transformer pare să nu se diminueze în următorii ani, este important să înțelegem cum funcționează și să o implementăm noi înșine, lucru pe care îl vom face în acest notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipQpJ9vY7PUH"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/lab12\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sj9329T7PUJ"
      },
      "source": [
        "Arhitectura Transformer\n",
        "\n",
        "În prima parte a acestui notebook, vom implementa arhitectura Transformer de la zero. Deși această arhitectură este atât de populară încât există deja un modul Pytorch, `nn.Transformer` ([documentație](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) și un [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) despre cum să îl utilizăm pentru predicția următorului token, vom implementa noi înșine această arhitectură pentru a înțelege cele mai mici detalii.\n",
        "\n",
        "Desigur, există multe alte tutoriale despre mecanismul de atenție și despre Transformeri. Mai jos, am inclus câteva resurse care merită explorate dacă ești interesat de acest subiect și ai dori o altă perspectivă după ce parcurgi acest material:\n",
        "\n",
        "- [Transformer: O arhitectură neuronală nouă pentru înțelegerea limbajului (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - Postarea originală pe blogul Google despre lucrarea Transformer, cu accent pe aplicațiile în traducerea automată.\n",
        "- [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - Un articol popular și excelent care explică intuitiv arhitectura Transformer cu multe vizualizări atractive. Accentul este pus pe NLP.\n",
        "- [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - Un articol care rezumă mecanismele de atenție în mai multe domenii, inclusiv viziunea computerizată.\n",
        "- [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - O vizualizare detaliată a pașilor procesului de auto-atenție. Recomandat dacă explicațiile de mai jos sunt prea abstracte pentru tine.\n",
        "- [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - Un articol detaliat care trece în revistă mai multe variante ale Transformerilor, dincolo de cel original.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ce este mecanismul de atenție?\n",
        "\n",
        "Mecanismul de atenție descrie un grup recent de straturi în rețelele neuronale care a atras mult interes în ultimii ani, mai ales în sarcinile legate de secvențe. Există multe definiții diferite ale \"atenției\" în literatură, dar cea pe care o vom folosi aici este următoarea: _mecanismul de atenție descrie o medie ponderată a elementelor (secvenței), cu ponderile calculate dinamic pe baza unei interogări (query) și a cheilor elementelor_. Ce înseamnă mai exact asta? Obiectivul este să calculăm o medie a caracteristicilor mai multor elemente. Totuși, în loc să ponderăm fiecare element în mod egal, dorim să le ponderăm în funcție de valorile lor reale. Cu alte cuvinte, dorim să decidem dinamic asupra căror intrări să \"acordăm atenție\" mai mult decât altora. În mod specific, un mecanism de atenție are de obicei patru părți pe care trebuie să le specificăm:\n",
        "\n",
        "* **Query (Interogare)**: Interogarea este un vector de caracteristici care descrie ce căutăm în secvență, adică la ce am dori să acordăm atenție.\n",
        "* **Keys (Chei)**: Pentru fiecare element de intrare, avem o cheie, care este din nou un vector de caracteristici. Acest vector descrie aproximativ ce \"oferă\" elementul sau când ar putea fi important. Cheile ar trebui proiectate astfel încât să putem identifica elementele asupra cărora dorim să acordăm atenție pe baza interogării.\n",
        "* **Values (Valori)**: Pentru fiecare element de intrare, avem și un vector de valori. Acest vector de caracteristici este cel asupra căruia dorim să calculăm media.\n",
        "* **Funcție de scor**: Pentru a evalua asupra căror elemente dorim să acordăm atenție, trebuie să specificăm o funcție de scor $f_{attn}$. Funcția de scor ia ca intrare interogarea și o cheie și returnează scorul/ponderea de atenție pentru perechea interogare-cheie. Este de obicei implementată prin metrici simple de similaritate, precum un produs scalar, sau un mic MLP.\n",
        "\n",
        "Ponderile mediei sunt calculate printr-un softmax peste toate rezultatele funcției de scor. Astfel, atribuim o pondere mai mare acelor vectori de valori ale căror chei corespund cel mai bine interogării. Dacă încercăm să descriem acest proces în pseudo-matematică, putem scrie:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "Vizual, putem arăta atenția aplicată asupra unei secvențe de cuvinte astfel:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "Pentru fiecare cuvânt, avem un vector de cheie și unul de valoare. Interogarea este comparată cu toate cheile printr-o funcție de scor (în acest caz, produsul scalar) pentru a determina ponderile. Softmax-ul nu este vizualizat pentru simplificare. În final, vectorii de valori ai tuturor cuvintelor sunt mediați utilizând ponderile de atenție.\n",
        "\n",
        "Majoritatea mecanismelor de atenție diferă prin ceea ce utilizează ca interogări, cum sunt definite cheile și vectorii de valori, și ce funcție de scor este folosită. Atenția aplicată în arhitectura Transformer se numește **auto-atenție (self-attention)**. În auto-atenție, fiecare element al secvenței oferă o cheie, o valoare și o interogare. Pentru fiecare element, aplicăm un strat de atenție în care, pe baza interogării sale, verificăm similaritatea tuturor cheilor elementelor secvenței și returnăm un vector de valori mediat diferit pentru fiecare element. Vom detalia acum mai specific implementarea mecanismului de atenție, care în cazul Transformer este atenția cu produs scalar scalat.\n"
      ],
      "metadata": {
        "id": "5LBmGz48joRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atenția cu Produs Scalar Scalată (Scaled Dot Product Attention)\n",
        "\n",
        "Conceptul de bază din spatele auto-atenției este atenția cu produs scalar scalată. Obiectivul nostru este să avem un mecanism de atenție prin care orice element dintr-o secvență să poată acorda atenție oricărui alt element, menținând în același timp eficiența de calcul. Atenția cu produs scalar primește ca intrare un set de interogări $Q\\in\\mathbb{R}^{T\\times d_k}$, chei $K\\in\\mathbb{R}^{T\\times d_k}$ și valori $V\\in\\mathbb{R}^{T\\times d_v}$, unde $T$ este lungimea secvenței, iar $d_k$ și $d_v$ sunt dimensiunile ascunse pentru interogări/chei și valori, respectiv. Pentru simplitate, neglijăm dimensiunea batch-ului deocamdată. Valoarea atenției de la elementul $i$ la $j$ este bazată pe similaritatea dintre interogarea $Q_i$ și cheia $K_j$, folosind produsul scalar ca metrică de similaritate. Matematic, calculăm atenția cu produs scalar astfel:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Multiplicarea matricială $QK^T$ efectuează produsul scalar pentru fiecare pereche posibilă de interogări și chei, rezultând o matrice cu forma $T\\times T$. Fiecare rând reprezintă logiturile de atenție pentru un element specific $i$ către toate celelalte elemente din secvență. Pe aceste valori aplicăm un softmax și multiplicăm cu vectorul de valori pentru a obține o medie ponderată (ponderile fiind determinate de atenție). O altă perspectivă asupra acestui mecanism de atenție este oferită de graficul de calcul vizualizat mai jos (credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "Un aspect pe care nu l-am discutat încă este factorul de scalare $1/\\sqrt{d_k}$. Acest factor de scalare este crucial pentru a menține o varianță adecvată a valorilor de atenție după inițializare. Reamintiți-vă că inițializăm straturile noastre cu intenția de a avea o varianță egală în tot modelul, așa că $Q$ și $K$ ar putea avea o varianță apropiată de $1$. Totuși, efectuarea unui produs scalar între doi vectori cu o varianță $\\sigma^2$ rezultă într-un scalar cu o varianță de $d_k$ ori mai mare:\n",
        "\n",
        "$$\n",
        "q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k\n",
        "$$\n",
        "\n",
        "Dacă nu reducem varianța înapoi la $\\sim\\sigma^2$, softmax-ul asupra logiturilor se va satura deja la $1$ pentru un element aleatoriu și la $0$ pentru toate celelalte. Gradienții prin softmax vor fi aproape zero, ceea ce împiedică învățarea corectă a parametrilor. Rețineți că factorul suplimentar $\\sigma^2$, adică având $\\sigma^4$ în loc de $\\sigma^2$, de obicei nu reprezintă o problemă, deoarece păstrăm varianța originală $\\sigma^2$ apropiată de $1$.\n",
        "\n",
        "Blocul `Mask (opt.)` din diagramă reprezintă mascarea opțională a anumitor intrări din matricea de atenție. Acest lucru este folosit, de exemplu, dacă stivuim mai multe secvențe cu lungimi diferite într-un batch. Pentru a beneficia în continuare de paralelizarea din PyTorch, completăm propozițiile la aceeași lungime și mascăm token-urile de completare în timpul calculului valorilor de atenție. De obicei, acest lucru se face setând logiturile de atenție respective la o valoare foarte mică.\n",
        "\n",
        "După ce am discutat detaliile blocului de atenție cu produs scalar scalat, putem scrie mai jos o funcție care calculează caracteristicile de ieșire, date tripletul de interogări, chei și valori:\n"
      ],
      "metadata": {
        "id": "fJHz47gWj5kE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGD7qVzV7PUK"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention computation.\n",
        "\n",
        "    Inputs:\n",
        "        q - Query tensor of shape (batch_size, num_heads, seq_length, head_dim)\n",
        "        k - Key tensor of shape (batch_size, num_heads, seq_length, head_dim)\n",
        "        v - Value tensor of shape (batch_size, num_heads, seq_length, head_dim)\n",
        "        mask - Optional mask tensor of shape (batch_size, 1, seq_length, seq_length) or broadcastable\n",
        "\n",
        "    Outputs:\n",
        "        values - Output tensor of shape (batch_size, num_heads, seq_length, head_dim)\n",
        "        attention - Attention weights of shape (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "    \"\"\"\n",
        "    # TODO 1.1 Compute attention logits:\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "    # TODO 1.2. Scale the attention logits:\n",
        "    d_k = q.size(-1)\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "\n",
        "    # TODO 1.3. Apply masking (if `mask` is provided):\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "\n",
        "    # TODO 1.4. Compute the attention weights:\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "\n",
        "    # TODO 1.5. Compute the final output values:\n",
        "    values = torch.matmul(attention, v)\n",
        "\n",
        "    # TODO 1.6. Return the results:\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1kpxKyK7PUK"
      },
      "source": [
        "Rețineți că codul nostru de mai sus suportă orice dimensiune suplimentară în fața lungimii secvenței, astfel încât să îl putem utiliza și pentru batch-uri. Totuși, pentru o înțelegere mai bună, să generăm câteva interogări, chei și vectori de valori aleatori și să calculăm ieșirile atenției:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD9ASYid7PUL"
      },
      "outputs": [],
      "source": [
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_scaled_dot_product_student():\n",
        "    # Define hardcoded inputs\n",
        "    q = torch.tensor([[ 0.3367,  0.1288],\n",
        "                      [ 0.2345,  0.2303],\n",
        "                      [-1.1229, -0.1863]])\n",
        "    k = torch.tensor([[ 2.2082, -0.6380],\n",
        "                      [ 0.4617,  0.2674],\n",
        "                      [ 0.5349,  0.8094]])\n",
        "    v = torch.tensor([[ 1.1103, -1.6898],\n",
        "                      [-0.9890,  0.9580],\n",
        "                      [ 1.3221,  0.8172]])\n",
        "\n",
        "    # Define expected outputs\n",
        "    expected_values = torch.tensor([[ 0.5698, -0.1520],\n",
        "                                     [ 0.5379, -0.0265],\n",
        "                                     [ 0.2246,  0.5556]])\n",
        "    expected_attention = torch.tensor([[0.4028, 0.2886, 0.3086],\n",
        "                                        [0.3538, 0.3069, 0.3393],\n",
        "                                        [0.1303, 0.4630, 0.4067]])\n",
        "\n",
        "    # Calculate attention using student's implementation\n",
        "    values_student, attention_student = scaled_dot_product(q, k, v)\n",
        "\n",
        "    # Compare outputs with expected values using torch.allclose()\n",
        "    assert torch.allclose(values_student, expected_values, atol=1e-4)\n",
        "    assert torch.allclose(attention_student, expected_attention, atol=1e-4)\n",
        "    print(\"TODO 1 Test Passed!\")"
      ],
      "metadata": {
        "id": "aF55jUqGbthq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_scaled_dot_product_student()"
      ],
      "metadata": {
        "id": "waorbDL6c2hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Atenția Multi-Head\n",
        "\n",
        "Atenția cu produs scalar scalat permite unei rețele să acorde atenție unei secvențe. Totuși, de multe ori există mai multe aspecte diferite asupra cărora un element al secvenței dorește să acorde atenție, iar o singură medie ponderată nu este o opțiune potrivită. De aceea extindem mecanismele de atenție la multiple heads, adică multiple triplete interogare-cheie-valoare aplicate pe aceleași caracteristici. Specific, având o matrice de interogări, chei și valori, le transformăm în $h$ sub-interogări, sub-chei și sub-valori, pe care le trecem independent prin mecanismul de atenție cu produs scalar scalat. Ulterior, concatenăm aceste heads și le combinăm cu o matrice finală de ponderi. Matematic, putem exprima această operație astfel:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{unde } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Acest strat este denumit strat de atenție Multi-Head, având parametrii antrenabili $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$ și $W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}$ ($D$ fiind dimensiunea de intrare). Exprimat într-un grafic computațional, putem vizualiza acest mecanism astfel (credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
        "\n",
        "Cum aplicăm un strat de atenție Multi-Head într-o rețea neuronală, unde nu avem vectori arbitari de interogare, cheie și valoare ca intrare? Privind graficul computațional de mai sus, o implementare simplă dar eficientă este să setăm harta de caracteristici curentă dintr-o rețea neuronală, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, ca $Q$, $K$ și $V$ ($B$ fiind dimensiunea batch-ului, $T$ lungimea secvenței, $d_{\\text{model}}$ dimensiunea ascunsă a lui $X$). Matricile consecutive de ponderi $W^{Q}$, $W^{K}$ și $W^{V}$ pot transforma $X$ în vectori de caracteristici corespunzători care reprezintă interogările, cheile și valorile intrării. Folosind această abordare, putem implementa modulul de Atenție Multi-Head mai jos.\n"
      ],
      "metadata": {
        "id": "gnmj_uEHlOGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKT3ACd97PUL"
      },
      "outputs": [],
      "source": [
        "# Helper function to support different mask shapes.\n",
        "# Output shape supports (batch_size, number of heads, seq length, seq length)\n",
        "# If 2D: broadcasted over batch size and number of heads\n",
        "# If 3D: broadcasted over number of heads\n",
        "# If 4D: leave as is\n",
        "def expand_mask(mask):\n",
        "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n",
        "    if mask.ndim == 3:\n",
        "        mask = mask.unsqueeze(1)\n",
        "    while mask.ndim < 4:\n",
        "        mask = mask.unsqueeze(0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIwgMjFp7PUL"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        \"\"\"\n",
        "        Forward pass of the MultiheadAttention module.\n",
        "\n",
        "        Inputs:\n",
        "            x - Input tensor of shape (batch_size, seq_length, input_dim)\n",
        "            mask - Optional mask tensor of shape (seq_length, seq_length) or broadcastable\n",
        "            return_attention - Boolean indicating whether to return attention weights along with the output\n",
        "\n",
        "        Outputs:\n",
        "            o - Output tensor of shape (batch_size, seq_length, embed_dim)\n",
        "            attention (optional) - Attention weights of shape (batch_size, num_heads, seq_length, seq_length) if `return_attention=True`\n",
        "\n",
        "        TODOs:\n",
        "        TODO 2.1. Project the input tensor `x` to produce Q, K, and V:\n",
        "        - Use `self.qkv_proj` to compute a single tensor of shape (batch_size, seq_length, 3*embed_dim).\n",
        "        - Split this tensor into Q, K, and V tensors of shape (batch_size, seq_length, embed_dim).\n",
        "\n",
        "        TODO 2.2. Reshape and permute Q, K, and V for multi-head processing:\n",
        "        - Reshape the tensors to shape (batch_size, seq_length, num_heads, head_dim), where `head_dim = embed_dim // num_heads`.\n",
        "        - Permute the tensors to shape (batch_size, num_heads, seq_length, head_dim) for computation.\n",
        "\n",
        "        TODO 2.3. Compute the scaled dot-product attention:\n",
        "        - Use the helper function `scaled_dot_product(q, k, v, mask=mask)` to compute:\n",
        "            - `values` of shape (batch_size, num_heads, seq_length, head_dim)\n",
        "            - `attention` weights of shape (batch_size, num_heads, seq_length, seq_length) (if `mask` is applied).\n",
        "        - The mask can be expanded using the `expand_mask` function, if provided.\n",
        "\n",
        "        TODO 2.4. Reshape and permute the attention output:\n",
        "        - Permute the `values` tensor back to shape (batch_size, seq_length, num_heads, head_dim).\n",
        "        - Reshape it to shape (batch_size, seq_length, embed_dim) for the output projection.\n",
        "\n",
        "        TODO 2.5. Apply the output projection:\n",
        "        - Use `self.o_proj` to project the reshaped `values` to the final output tensor of shape (batch_size, seq_length, embed_dim).\n",
        "\n",
        "        TODO 2.6. Return the outputs:\n",
        "        - If `return_attention` is `True`, return both the output tensor and the attention weights.\n",
        "        - Otherwise, return only the output tensor.\n",
        "\n",
        "        Hint:\n",
        "        - Use the `qkv.chunk(3, dim=-1)` method to split Q, K, and V from the concatenated tensor.\n",
        "        - Ensure the input-output shapes align throughout the computation.\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # TODO 2.1. Project the input tensor `x` to produce Q, K, and V:\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # TODO 2.2. Reshape and permute Q, K, and V for multi-head processing:\n",
        "        q = q.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.reshape(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = expand_mask(mask)\n",
        "\n",
        "        # TODO 2.3. Compute the scaled dot-product attention:\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "\n",
        "        # TODO 2.4. Reshape and permute the attention output:\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, seq_length, self.embed_dim)\n",
        "\n",
        "        # TODO 2.5. Apply the output projection:\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        # TODO 2.6. Return the outputs:\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IG0UZD17PUM"
      },
      "source": [
        "O caracteristică crucială a atenției multi-head este că este permutabil-ecuativă față de intrările sale. Asta înseamnă că, dacă schimbăm două elemente de intrare din secvență, de exemplu $X_1\\leftrightarrow X_2$ (neglijând dimensiunea batch-ului pentru moment), ieșirea va fi exact aceeași, cu excepția elementelor 1 și 2 schimbate între ele. Astfel, atenția multi-head privește intrarea nu ca o secvență, ci ca un set de elemente. Această proprietate face ca blocul de atenție multi-head și arhitectura Transformer să fie atât de puternice și aplicabile pe scară largă! Dar ce se întâmplă dacă ordinea intrării este de fapt importantă pentru rezolvarea unei sarcini, cum ar fi modelarea limbajului? Răspunsul este să codificăm poziția în caracteristicile de intrare, lucru pe care îl vom analiza mai detaliat mai târziu (subiectul _Codificări poziționale_ mai jos).\n",
        "\n",
        "Înainte de a trece la crearea arhitecturii Transformer, putem compara operația de auto-atenție cu alte straturi comune pentru date secvențiale: convoluțiile și rețelele neuronale recurente. Mai jos puteți găsi un tabel realizat de [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) care prezintă complexitatea per strat, numărul de operații secvențiale și lungimea maximă a căii. Complexitatea este măsurată prin limita superioară a numărului de operații necesare, în timp ce lungimea maximă a căii reprezintă numărul maxim de pași pe care un semnal de avans sau de propagare inversă trebuie să-i parcurgă pentru a ajunge la orice altă poziție. Cu cât această lungime este mai mică, cu atât mai bine pot semnalele gradientului să se propage pentru dependențe pe termen lung. Să analizăm tabelul de mai jos:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/comparison_conv_rnn.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "$n$ este lungimea secvenței, $d$ este dimensiunea reprezentării, iar $k$ este dimensiunea kernelului convoluțiilor. Spre deosebire de rețelele neuronale recurente, stratul de auto-atenție poate paraleliza toate operațiile sale, făcându-l mult mai rapid pentru secvențe mai scurte. Totuși, atunci când lungimea secvenței depășește dimensiunea ascunsă, auto-atenția devine mai costisitoare decât RNN-urile. O modalitate de a reduce costul computațional pentru secvențe lungi este restricționarea auto-atenției la un vecinaj de intrări asupra cărora să acorde atenție, notat cu $r$. Cu toate acestea, recent s-a lucrat mult la arhitecturi Transformer mai eficiente, care încă permit dependențe pe termen lung. Puteți găsi o prezentare generală a acestor lucrări în articolul realizat de [Tay et al. (2020)](https://arxiv.org/abs/2009.06732), dacă sunteți interesați."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder-ul Transformer\n",
        "\n",
        "În continuare, vom analiza cum să aplicăm blocul de atenție multi-head în cadrul arhitecturii Transformer. Inițial, modelul Transformer a fost conceput pentru traducerea automată. Astfel, are o structură encoder-decoder în care encoder-ul primește ca intrare propoziția în limba originală și generează o reprezentare bazată pe atenție. Pe de altă parte, decoder-ul analizează informația codificată și generează propoziția tradusă într-un mod autoregresiv, similar cu o RNN standard. Deși această structură este extrem de utilă pentru sarcini de tip Secvență-la-Secvență care necesită decodare autoregresivă, ne vom concentra aici pe partea de encoder. Multe progrese în procesarea limbajului natural (NLP) au fost realizate folosind modele Transformer bazate doar pe encoder (exemple includ familia [BERT](https://arxiv.org/abs/1810.04805), [Vision Transformer](https://arxiv.org/abs/2010.11929) și altele). În cadrul tutorialului nostru, ne vom concentra în principal pe partea de encoder. Dacă înțelegeți arhitectura encoder-ului, implementarea decoder-ului reprezintă un pas mic suplimentar. Arhitectura completă a Transformer-ului arată astfel (credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "Encoder-ul constă în $N$ blocuri identice aplicate în secvență. Având ca intrare $x$, aceasta este mai întâi trecută printr-un bloc de atenție multi-head, așa cum am implementat mai sus. Ieșirea este adăugată la intrarea originală utilizând o conexiune reziduală, iar suma este trecută printr-o Normalizare de Strat (Layer Normalization). Per ansamblu, se calculează $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ fiind $Q$, $K$ și $V$ pentru stratul de atenție). Conexiunea reziduală este crucială în arhitectura Transformer din două motive:\n",
        "\n",
        "1. Similar cu ResNet-urile, Transformer-ele sunt proiectate să fie foarte adânci. Unele modele conțin mai mult de 24 de blocuri în encoder. Astfel, conexiunile reziduale sunt esențiale pentru a permite un flux neted al gradientului prin model.\n",
        "2. Fără conexiunea reziduală, informația despre secvența originală se pierde. Amintiți-vă că stratul de atenție Multi-Head ignoră poziția elementelor într-o secvență și poate învăța aceasta doar pe baza caracteristicilor de intrare. Eliminarea conexiunilor reziduale ar însemna pierderea acestei informații după primul strat de atenție (după inițializare), iar cu un vector de interogare și cheie inițializat aleatoriu, ieșirile pentru poziția $i$ nu ar avea nicio legătură cu intrarea originală. Toate ieșirile stratului de atenție ar reprezenta probabil informații similare, iar modelul nu ar putea distinge ce informații provin de la fiecare element de intrare. O alternativă la conexiunea reziduală ar fi fixarea unui head pentru a se concentra pe intrarea sa originală, dar aceasta este foarte ineficientă și nu beneficiază de îmbunătățirea fluxului gradientului.\n",
        "\n",
        "Normalizarea de Strat joacă, de asemenea, un rol important în arhitectura Transformer, deoarece permite o antrenare mai rapidă și oferă o ușoară regularizare. În plus, asigură că caracteristicile au o magnitudine similară între elementele secvenței. Nu utilizăm Normalizarea pe Batch deoarece aceasta depinde de dimensiunea batch-ului, care este adesea mică în cazul Transformer-elor (necesită multă memorie GPU), iar BatchNorm s-a dovedit a performa slab în limbaj, deoarece caracteristicile cuvintelor tind să aibă o varianță mult mai mare (există multe cuvinte foarte rare care trebuie considerate pentru o estimare bună a distribuției).\n",
        "\n",
        "Pe lângă atenția multi-head, un mic feed-forward network complet conectat este adăugat modelului, aplicat separat și identic pe fiecare poziție. Specific, modelul folosește un MLP de tip Linear$\\to$ReLU$\\to$Linear. Transformarea completă, inclusiv conexiunea reziduală, poate fi exprimată astfel:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Acest MLP adaugă complexitate suplimentară modelului și permite transformări pe fiecare element al secvenței separat. Vă puteți imagina că aceasta permite modelului să \"prelucreze\" noile informații adăugate de atenția multi-head anterioară și să le pregătească pentru următorul bloc de atenție. De obicei, dimensiunea interioară a MLP-ului este de 2-8$\\times$ mai mare decât $d_{\\text{model}}$, adică dimensiunea intrării originale $x$. Avantajul general al unui strat mai larg în locul unui MLP îngust, multi-strat, este execuția mai rapidă și paralelizabilă.\n",
        "\n",
        "În final, după ce am analizat toate părțile arhitecturii encoder-ului, putem începe implementarea acestuia mai jos. Vom începe prin implementarea unui singur bloc de encoder. Pe lângă straturile descrise mai sus, vom adăuga dropout în cadrul MLP-ului și pe ieșirea MLP-ului și a atenției multi-head pentru regularizare.\n"
      ],
      "metadata": {
        "id": "Q-TZML20lyga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYUq_OCA7PUM"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the EncoderBlock.\n",
        "\n",
        "        Inputs:\n",
        "            x - Input tensor of shape (seq_len, batch_size, input_dim)\n",
        "            mask - Optional mask tensor of shape (seq_len, seq_len) to prevent attention to certain positions\n",
        "\n",
        "        Outputs:\n",
        "            Output tensor of the same shape as input (seq_len, batch_size, input_dim)\n",
        "\n",
        "        TODOs:\n",
        "        TODO3.1. Implement the self-attention mechanism:\n",
        "        - Use `self.self_attn` to compute the attention output.\n",
        "        - Pass the input tensor `x` and the optional `mask` as arguments.\n",
        "        - The output shape of the attention mechanism should match `x` (seq_len, batch_size, input_dim).\n",
        "\n",
        "        TODO3.2. Add residual connection:\n",
        "        - Add the attention output to the original input tensor `x`.\n",
        "        - Use the `self.dropout` layer on the attention output before adding it.\n",
        "\n",
        "        TODO3.3. Apply layer normalization:\n",
        "        - Use `self.norm1` to normalize the resulting tensor after the residual connection.\n",
        "\n",
        "        TODO3.4. Implement the feedforward network:\n",
        "        - Pass the normalized tensor through `self.linear_net` to compute the output of the feedforward network.\n",
        "        - The feedforward network processes the tensor of shape (seq_len, batch_size, input_dim) and returns a tensor of the same shape.\n",
        "\n",
        "        TODO3.5. Add another residual connection:\n",
        "        - Add the output of the feedforward network to its input.\n",
        "        - Use `self.dropout` on the feedforward network output before adding it.\n",
        "\n",
        "        TODO3.6. Apply the second layer normalization:\n",
        "        - Use `self.norm2` to normalize the final output tensor.\n",
        "\n",
        "        Make sure to preserve the input-output shapes throughout the forward method.\n",
        "        \"\"\"\n",
        "        # TODO3.1. Implement the self-attention mechanism:\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "\n",
        "        # TODO3.2. Add residual connection:\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        # TODO3.3. Apply layer normalization:\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # TODO3.4. Implement the feedforward network:\n",
        "        linear_out = self.linear_net(x)\n",
        "\n",
        "        # TODO3.5. Add another residual connection:\n",
        "        x = x + self.dropout(linear_out)\n",
        "\n",
        "        # TODO3.6. Apply the second layer normalization:\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hqSSvx_7PUM"
      },
      "source": [
        "Pe baza acestui bloc, putem implementa un modul pentru encoder-ul complet al Transformer-ului. Pe lângă o funcție `forward` care iterează prin secvența de blocuri de encoder, oferim și o funcție numită `get_attention_maps`. Ideea acestei funcții este să returneze probabilitățile de atenție pentru toate blocurile de atenție multi-head din encoder. Acest lucru ne ajută să înțelegem și, într-un fel, să explicăm modelul. Totuși, probabilitățile de atenție ar trebui interpretate cu prudență, deoarece nu reflectă neapărat adevărata interpretare a modelului (există o serie de lucrări despre acest subiect, inclusiv [Attention is not Explanation](https://arxiv.org/abs/1902.10186) și [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd_fBc2h7PUN"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fC_ZgQ67PUN"
      },
      "source": [
        "### Codificare pozițională\n",
        "\n",
        "Am discutat anterior că blocul de atenție multi-head este permutabil-ecuativ și nu poate distinge dacă o intrare apare înaintea alteia într-o secvență. Totuși, în sarcini precum înțelegerea limbajului, poziția este importantă pentru interpretarea cuvintelor de intrare. Informația despre poziție poate fi adăugată prin caracteristicile de intrare. Am putea învăța o reprezentare pentru fiecare poziție posibilă, dar aceasta nu s-ar generaliza la o lungime dinamică a secvenței de intrare. Prin urmare, opțiunea mai bună este să folosim modele de caracteristici pe care rețeaua le poate identifica și generaliza la secvențe mai lungi. Modelul specific ales de Vaswani et al. constă în funcții sinusoidale și cosinusoidale de frecvențe diferite, astfel:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{dacă}\\hspace{3mm} i \\text{ mod } 2=0\\\\\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{altfel}\\\\\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ reprezintă codificarea pozițională la poziția $pos$ în secvență, și dimensiunea ascunsă $i$. Aceste valori, concatenate pentru toate dimensiunile ascunse, sunt adăugate caracteristicilor de intrare originale (în vizualizarea Transformer de mai sus, vezi \"Positional encoding\") și constituie informația despre poziție. Distincția se face între dimensiunile ascunse pare ($i \\text{ mod } 2=0$) și impare ($i \\text{ mod } 2=1$), unde aplicăm respectiv funcția sinus/cosinus. Intuiția din spatele acestei codificări este că poți reprezenta $PE_{(pos+k,:)}$ ca o funcție liniară a lui $PE_{(pos,:)}$, ceea ce ar putea permite modelului să analizeze ușor pozițiile relative. Lungimile de undă din dimensiunile diferite variază între $2\\pi$ și $10000\\cdot 2\\pi$.\n",
        "\n",
        "Codificarea pozițională este implementată mai jos. Codul este preluat din [tutorialul PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) despre Transformeri în NLP și ajustat pentru scopurile noastre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbyIj0mw7PUN"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXn5_A-7PUN"
      },
      "source": [
        "*Pentru* a înțelege codificarea pozițională, o putem vizualiza mai jos. Vom genera o imagine a codificării poziționale pe dimensiunea ascunsă și poziția într-o secvență. Fiecare pixel reprezintă astfel modificarea caracteristicii de intrare pe care o realizăm pentru a codifica o anumită poziție. Să procedăm mai jos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIrOUzIR7PUN"
      },
      "outputs": [],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttXHvZnH7PUO"
      },
      "source": [
        "Așa cum putem observa, modelele dintre dimensiunile ascunse $1$ și $2$ diferă doar prin unghiul de start. Lungimea de undă este $2\\pi$, deci repetarea apare după poziția $6$. Dimensiunile ascunse $2$ și $3$ au aproximativ de două ori lungimea de undă."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBnUJvj7PUO"
      },
      "source": [
        "### Încălzirea ratei de învățare\n",
        "\n",
        "O tehnică frecvent utilizată pentru antrenarea unui Transformer este încălzirea ratei de învățare. Aceasta înseamnă că creștem treptat rata de învățare de la 0 până la rata specificată inițial, în primele câteva iterații. Astfel, începem procesul de învățare încet, în loc să facem pași foarte mari de la început. De fapt, antrenarea unui Transformer profund fără încălzirea ratei de învățare poate face ca modelul să diverge și să obțină performanțe mult mai slabe la antrenare și testare. Pentru mai multe informatii studiati graficul realizat de [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) care compară Adam-vanilla (i.e., Adam fără încălzire) cu Adam cu încălzire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KjAz7kz7PUP"
      },
      "outputs": [],
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "\n",
        "    def __init__(self, optimizer, warmup, max_iters):\n",
        "        self.warmup = warmup\n",
        "        self.max_num_iters = max_iters\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "\n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML6aaZkr7PUP"
      },
      "outputs": [],
      "source": [
        "# Needed for initializing the lr scheduler\n",
        "p = nn.Parameter(torch.empty(4,4))\n",
        "optimizer = optim.Adam([p], lr=1e-3)\n",
        "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
        "\n",
        "# Plotting\n",
        "epochs = list(range(2000))\n",
        "sns.set()\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
        "plt.ylabel(\"Learning rate factor\")\n",
        "plt.xlabel(\"Iterations (in batches)\")\n",
        "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
        "plt.show()\n",
        "sns.reset_orig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT_Smkr-7PUP"
      },
      "source": [
        "În primele 100 de iterații, creștem factorul ratei de învățare de la 0 la 1, în timp ce pentru toate iterațiile ulterioare îl reducem folosind unda cosinusoidală. Implementări predefinite ale acestui scheduler pot fi găsite în biblioteca populară NLP Transformer [huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine#transformers.get_cosine_schedule_with_warmup)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV7_bc307PUP"
      },
      "source": [
        "### Modul PyTorch Lightning\n",
        "\n",
        "În final, putem integra arhitectura Transformer într-un modul PyTorch Lightning. Vom implementa un șablon pentru un clasificator bazat pe encoder-ul Transformer. Astfel, avem o ieșire de predicție per element al secvenței. Dacă am avea nevoie de un clasificator pentru întreaga secvență, abordarea obișnuită este să adăugăm un token suplimentar `[CLS]` în secvență, care să reprezinte token-ul de clasificare. Totuși, aici ne concentrăm pe sarcini în care avem o ieșire per element.\n",
        "\n",
        "Pe lângă arhitectura Transformer, adăugăm o rețea mică de intrare (care mapează dimensiunile de intrare la dimensiunile modelului), codificarea pozițională și o rețea de ieșire (care transformă codificările de ieșire în predicții). Adăugăm, de asemenea, scheduler-ul pentru rata de învățare, care face un pas la fiecare iterație în loc de o dată pe epocă. Acest lucru este necesar pentru încălzirea și scăderea lină cosinusoidală. Pașii pentru antrenare, validare și testare sunt lăsați goi pentru moment și vor fi completați pentru modelele noastre specifice sarcinii."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7UJKwXD7PUP"
      },
      "outputs": [],
      "source": [
        "class TransformerPredictor(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Hidden dimensionality of the input\n",
        "            model_dim - Hidden dimensionality to use inside the Transformer\n",
        "            num_classes - Number of classes to predict per sequence element\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers - Number of encoder blocks to use.\n",
        "            lr - Learning rate in the optimizer\n",
        "            warmup - Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout - Dropout to apply inside the model\n",
        "            input_dropout - Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        # Input dim -> Model dim\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Dropout(self.hparams.input_dropout),\n",
        "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
        "        )\n",
        "        # Positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
        "        # Transformer\n",
        "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
        "                                              input_dim=self.hparams.model_dim,\n",
        "                                              dim_feedforward=2*self.hparams.model_dim,\n",
        "                                              num_heads=self.hparams.num_heads,\n",
        "                                              dropout=self.hparams.dropout)\n",
        "        # Output classifier per sequence lement\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(self.hparams.dropout),\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask - Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "        # Apply lr scheduler per step\n",
        "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
        "                                             warmup=self.hparams.warmup,\n",
        "                                             max_iters=self.hparams.max_iters)\n",
        "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT49vrsK7PUP"
      },
      "source": [
        "## Experimente\n",
        "\n",
        "După finalizarea implementării arhitecturii Transformer, putem începe să experimentăm și să o aplicăm pe diverse sarcini. În acest notebook, ne vom concentra pe două sarcini: traducerea paralelă Secvență-la-Secvență și detectarea anomaliilor într-un set. Cele două sarcini se concentrează pe proprietăți diferite ale arhitecturii Transformer, pe care le vom analiza mai jos.\n",
        "\n",
        "### Sequence-to-sequence\n",
        "\n",
        "O sarcină Sequence-to-sequence reprezintă o sarcină în care atât intrarea _cât și_ ieșirea sunt secvențe, nu neapărat de aceeași lungime. Sarcini populare în acest domeniu includ traducerea automată și sumarizarea. Pentru acestea, de obicei avem un encoder Transformer pentru interpretarea secvenței de intrare și un decoder pentru generarea ieșirii într-un mod autoregresiv. Totuși, aici vom reveni la o sarcină mult mai simplă și vom utiliza doar encoder-ul. Având o secvență de $N$ numere între $0$ și $M$, sarcina este să inversăm secvența de intrare. În notația Numpy, dacă intrarea este $x$, ieșirea ar trebui să fie $x$[::-1]. Deși această sarcină pare foarte simplă, RNN-urile pot avea dificultăți din cauza dependențelor pe termen lung necesare. Transformer-ele sunt construite pentru a sprijini astfel de dependențe și, prin urmare, ne așteptăm să funcționeze foarte bine.\n",
        "\n",
        "Mai întâi, să creăm o clasă pentru dataset mai jos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxiEk6AV7PUP"
      },
      "outputs": [],
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        return inp_data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mclvrU9v7PUQ"
      },
      "source": [
        "Creăm un număr arbitrar de secvențe aleatoare de numere între 0 și `num_categories-1`. Eticheta este pur și simplu tensorul inversat pe dimensiunea secvenței. Putem crea mai jos data loaders corespunzătoare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZjJLL7d7PUQ"
      },
      "outputs": [],
      "source": [
        "dataset = partial(ReverseDataset, 10, 16)\n",
        "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4GdvtQJ7PUQ"
      },
      "source": [
        "Un sample arbitrar din dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HKtt7Tf7PUQ"
      },
      "outputs": [],
      "source": [
        "inp_data, labels = train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdA3k5qL7PUQ"
      },
      "source": [
        "În timpul antrenării, trecem secvența de intrare prin encoder-ul Transformer și prezicem ieșirea pentru fiecare token de intrare. Folosim pierderea standard Cross-Entropy pentru a realiza acest lucru. Fiecare număr este reprezentat ca un vector one-hot. Rețineți că reprezentarea categoriilor ca scalari simpli reduce extrem de mult expresivitatea modelului, deoarece $0$ și $1$ nu sunt mai apropiate decât $0$ și $9$ în exemplul nostru. O alternativă la un vector one-hot este utilizarea unui vector de embedding antrenabil, oferit de modulul PyTorch `nn.Embedding`. Totuși, utilizarea unui vector one-hot cu un strat linear suplimentar, ca în cazul nostru, are același efect ca un strat de embedding (`self.input_net` mapează vectorul one-hot la un vector dens, unde fiecare rând al matricei de greutăți reprezintă embedding-ul pentru o categorie specifică).\n",
        "\n",
        "Pentru a implementa dinamica antrenării, creăm o nouă clasă care moștenește `TransformerPredictor` și suprascrie funcțiile pentru pașii de antrenare, validare și testare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsHztxlG7PUQ"
      },
      "outputs": [],
      "source": [
        "class ReversePredictor(TransformerPredictor):\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data, labels = batch\n",
        "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Logging\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss, acc\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXM6M2Y7PUR"
      },
      "source": [
        "În final, putem crea o funcție de antrenare similară cu cea pe care am văzut-o în Tutorialul 5 pentru PyTorch Lightning. Creăm un obiect `pl.Trainer`, care rulează pentru $N$ epoci, loghează în TensorBoard și salvează cel mai bun model bazat pe validare. Ulterior, testăm modelele noastre pe setul de testare. Un parametru suplimentar pe care îl transmitem aici trainer-ului este `gradient_clip_val`. Acesta taie norma gradientelor pentru toți parametrii înainte de a face un pas cu optimizatorul și previne modelul să diverge dacă obținem gradienti foarte mari, de exemplu, pe suprafețe ascuțite de pierdere (consultați numeroasele articole bune despre gradient clipping, precum [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). Pentru Transformeri, gradient clipping poate ajuta la stabilizarea suplimentară a antrenării în primele iterații și ulterior. În PyTorch simplu, puteți aplica gradient clipping prin `torch.nn.utils.clip_grad_norm_(...)` (vezi [documentația](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)). Valoarea de tăiere este de obicei între 0.5 și 10, în funcție de cât de agresiv doriți să reduceți gradientele mari. După ce am explicat acest lucru, să implementăm funcția de antrenare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvDazuHw7PUR"
      },
      "outputs": [],
      "source": [
        "def train_reverse(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=10,\n",
        "                         gradient_clip_val=5)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUD5wyu_7PUR"
      },
      "source": [
        "În final, putem antrena modelul. În această configurare, vom folosi un singur bloc de encoder și un singur head în cadrul Multi-Head Attention. Această alegere este făcută datorită simplității sarcinii, iar în acest caz, atenția poate fi interpretată ca o \"explicație\" a predicțiilor (comparativ cu lucrările menționate mai sus care abordează Transformeri adânci)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urAZ3unT7PUS"
      },
      "outputs": [],
      "source": [
        "reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
        "                                              model_dim=32,\n",
        "                                              num_heads=1,\n",
        "                                              num_classes=train_loader.dataset.num_categories,\n",
        "                                              num_layers=1,\n",
        "                                              dropout=0.0,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyMeqPtB7PUS"
      },
      "source": [
        "Avertismentul din PyTorch Lightning referitor la numărul de workeri poate fi ignorat pentru moment. Deoarece setul de date este atât de simplu, iar metoda `__getitem__` este finalizată într-un timp neglijabil, nu avem nevoie de subprocese pentru a ne furniza datele (de fapt, un număr mai mare de workeri poate încetini antrenarea din cauza supraîncărcării comunicării între procese/threads). Mai întâi, să afișăm rezultatele:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hia3Q1f_7PUS"
      },
      "outputs": [],
      "source": [
        "print(f\"Val accuracy:  {(100.0 * reverse_result['val_acc']):4.2f}%\")\n",
        "print(f\"Test accuracy: {(100.0 * reverse_result['test_acc']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADBXPcv7PUS"
      },
      "source": [
        "Așa cum ne-am fi așteptat, Transformer-ul poate rezolva corect sarcina. Totuși, cum arată atenția în cadrul blocului de Multi-Head Attention pentru o intrare arbitrară? Să încercăm să o vizualizăm mai jos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQNjoIpN7PUS"
      },
      "outputs": [],
      "source": [
        "data_input, labels = next(iter(val_loader))\n",
        "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
        "inp_data = inp_data.to(device)\n",
        "attention_maps = reverse_model.get_attention_maps(inp_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loAPO0a37PUT"
      },
      "source": [
        "Obiectul `attention_maps` este o listă de lungime $N$, unde $N$ este numărul de straturi. Fiecare element este un tensor de formă [Batch, Heads, SeqLen, SeqLen], lucru pe care îl putem verifica mai jos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaaLQuEF7PUT"
      },
      "outputs": [],
      "source": [
        "attention_maps[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gac6ZJZW7PUT"
      },
      "source": [
        "În continuare, vom scrie o funcție de graficare care primește ca intrare secvențele, hărțile de atenție și un indice care indică pentru care element din batch dorim să vizualizăm harta de atenție. Vom crea un grafic în care, pe rânduri, avem straturile diferite, iar pe coloane, afișăm head-urile diferite. Rețineți că softmax-ul a fost aplicat separat pentru fiecare rând."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59zmQotd7PUT"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
        "\n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    seq_len = input_data.shape[0]\n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(input_data.tolist())\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(input_data.tolist())\n",
        "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69hCGXB27PUT"
      },
      "source": [
        "În final, putem reprezenta grafic harta de atenție a Transformer-ului nostru antrenat pe sarcina de inversare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rTwVuOy7PUU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(data_input, attention_maps, idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmaNyPVu7PUU"
      },
      "source": [
        "Modelul a învățat să acorde atenție token-ului care se află pe indexul inversat al său. Prin urmare, face exact ceea ce am intenționat să facă. Observăm totuși că acordă o anumită atenție și valorilor apropiate de indexul inversat. Acest lucru se întâmplă deoarece modelul nu are nevoie de o atenție perfectă și strictă pentru a rezolva această problemă, ci este suficient cu această hartă de atenție aproximativă și zgomotoasă. Indicii apropiați sunt cauzați de similaritatea codificării poziționale, lucru pe care l-am intenționat și cu codificarea pozițională."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YsXcCv-7PUa"
      },
      "source": [
        "## Concluzie\n",
        "\n",
        "În acest tutorial, am analizat mai îndeaproape stratul de atenție Multi-Head, care utilizează un produs scalar scalat între interogări (queries) și chei (keys) pentru a găsi corelații și similarități între elementele de intrare. Arhitectura Transformer se bazează pe stratul de atenție Multi-Head și aplică mai multe astfel de straturi într-un bloc similar ResNet. Transformer-ul este o arhitectură foarte importantă și recentă, care poate fi aplicată pe multe sarcini și seturi de date. Deși este cel mai bine cunoscut pentru succesul său în NLP, este utilizabil în multe alte domenii. Am văzut aplicarea sa pe sarcini de tip secvență-la-secvență și pe detectarea anomaliilor într-un set. Proprietatea sa de a fi permutabil-ecuativ, în absența codificărilor poziționale, îi permite să se generalizeze în multe contexte. Prin urmare, este important să cunoaștem arhitectura, dar și posibilele sale probleme, cum ar fi problema gradientului în primele iterații, rezolvată prin încălzirea ratei de învățare.\n",
        "\n",
        "Dacă sunteți interesați să continuați studiul arhitecturii Transformer, vă rugăm să consultați articolele de blog enumerate la începutul acestui tutorial."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}